{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preguntas conceptuales\n",
    "\n",
    "1. Hay varias respuestas posibles: \n",
    "     * *versión simplista*: La entropía la definimos inicialmente como una que midiera el número de estados compatibles con un estado promedio. \n",
    "     * *Versión termodinámica* : La entropía crece cuando quitamos constricciones. Esto pasa incluso cuando el resto de las variables termodinámicas se mantengan constantes. De hecho sólo se mantiene constante la entropía en procesos reversibles. Por otra parte quitar constricciones implica un mayor número de configuraciones posibles. Así, la entropía aumenta al aumentar el número de configuraciones. Lo que implica que la entropía no es simplemente un promedio y de hecho mide qué tan probable es que el promedio represente la situación local. \n",
    "     * *Versión de definición de calor*: El calor lo definimos de dos formas. En un estado en equilibrio, el calor es: $dQ = TdS$. Si se mantiene la temperatura constante (una de las variables que es un promedio), el calór aún puede aumentar, si la entropía aumenta. Por otra parte, el calor lo definimos como todo aquello que cambia la energía interna del sistema por interacciones microscópicas que no tienen un efecto macroscópico. Es decir, la medida de la entropía nos mide cuanta información perdemos en los promedios. \n",
    "     * *Versión estadística*: La entropía está definida estadísticamente como $S = k log(\\Omega)$ donde $\\Omega$ es el número de configuraciones de microestados. Por lo tanto, S mide qué tanto se pierde al hacer promedios, pues los promedios son sobre $\\Omega$. \n",
    "2. Las 3 propiedades de la entropía son: $S \\geq 0$, $S$ es aditiva y $S$ aumenta si se quitan constricciones al sistema. Equivalentenemte, el hecho de que $S$ aumente al quitar constricciones, es igual a que $S$ sea concava (y se perdona si dijeron convexa (error que quizá yo mismo tuve)). \n",
    "\n",
    "# Problemas: \n",
    "\n",
    "I. Primero queremos calcular la probabilidad de que $n$ personas cumplan todos diferente día, para el caso donde solo hay un año de $365$ días. El número de configuraciones de días de cumpleaños en un año de 365 días, con $n$ personas es $365^n$, pues cada persona tiene $365$ días como posibilidad de cumpleaños, así, si se tiene una persona nada más, hay $365$ posibilidades, 2 personas serían $365$ por cada día de la primera persona, esto es $365^2$ y así sucesivamente, hasta $n$ personas. Entonces $\\Omega = 365^n$. \n",
    "\n",
    "El número de configuraciones de $n$ personas, donde todos cumplan un día diferente, es el número de permutaciones de $n$ números diferentes en $365$ objetos. Esto es: $\\Omega(E(n)) = \\frac{365!}{(365-n)!}$. Por lo tanto la probabilidad de tener $n$ cumpleaños diferentes en un año, es simplemente: \n",
    "\n",
    "$\\frac{\\Omega}{\\Omega(E(n))} = \\frac{365!}{365^n (365-n)!}$\n",
    "\n",
    "Si ahora pensamos en 3 años, pero excluimos el día bisiesto, la probabilidad de que $n$ nazcan en diferentes días del año es la misma, pues cada día sigue siendo igualmente probable. En este caso, sin embargo, el número de configuraciones $\\Omega$ cambia por $(3\\cdot 363)^n$. Reescribiendo la ecuación de arriba, tenemos: \n",
    "\n",
    "$\\frac{\\Omega}{\\Omega(E(n))} = \\frac{365!3^n}{(365\\cdot 3)^n (365-n)!}$, \n",
    "\n",
    "de donde identificamos que $\\Omega(E(n))=\\frac{365!3^n}{(365-n)!}$. Aquí $\\Omega(E(n))$ representa el número de configuraciones de $n$ cumpleaños diferentes, en 3 años (donde ningún día es bisiesto). Así, ya contamos todas las configuraciones de $n$ cumpleaños en 3 años, si ninguna persona cumple años en día bisiesto. Falta contar sólo el caso donde uno cumple en día bisiesto. Si más de uno cumplieran en día bisiesto, entonces al menos 2 personas cumplirían el mismo día y por ahora no queremos calcular ese caso. Si uno cumple en día bisiesto, $n-1$ cumplen en los otros 365 días del año. Así que podemos utilizar la fórmula para  $n-1$ personas. Esto es: $\\Omega(E(n-1))=\\frac{365!3^{n-1}}{(366-n)!}$. Pero puesto que hay $n$ personas que podrían haber nacido el día bisiesto, esto hay que multiplicarlo por $n$. Es decir, el número total de configuraciones es: $\\Omega(E(n))+n\\Omega(E(n-1))$\n",
    "\n",
    "Por otra parte, $\\Omega = (3\\cdot 365 +1)^n$ es el número de configuraciones totales de 3 años considerando uno de esos años como año bisiesto. Lo que implica que la probabilidad de que todos nazcan diferentes días es: \n",
    "\n",
    "$P(n) = \\frac{\\Omega(E(n))+n\\Omega(E(n-1))}{\\Omega} = \\frac{n 365!3^{n-1}}{(366-n)!(3\\cdot 365 +1)^n} + \\frac{365!3^n}{(365-n)!(3\\cdot 365 +1)^n} = \\frac{365! 3^{n-1}(3\\cdot 366 -2n)}{(366-n)!}$\n",
    "\n",
    "O bien, la probabilidad de que al menos 2 nazcan el mismo día es: \n",
    "\n",
    "$1-P(n) = 1-  \\frac{365! 3^{n-1}(3\\cdot 366 -2n)}{(366-n)!}$\n",
    "\n",
    "Sustituyendo $n$ por 25 se obtiene el resultado buscado. \n",
    "\n",
    "Aproximadamente es $0.568$ (esto último no era necesario calcularlo en el examen, pues no tenían calculadoras). \n",
    "\n",
    "II. \n",
    "i) El desplazamiento cuadrático medio es: $\\langle (x_0 -x)^2 \\rangle$, donde $x_0$ es la posición inicial, que consideraremos 0 y $x$ es la posición en el paso $n$. Por otro lado, la varianza es $var(x) = \\sigma ^2 = \\langle x^2 \\rangle - \\langle x \\rangle^2$. Pero por la simetría del sistema, $\\langle x \\rangle = 0$. Por lo que $ \\langle x^2 \\rangle = \\sigma ^2$. Pero $x$ sigue una distribución binomial, por lo que podemos aproximarlo por una gaussiana. En esa aproximación, $\\sigma = \\sqrt{n p (1-p)}$ por lo que $\\langle x^2 \\rangle \\sim n $.\n",
    "\n",
    "(fuera de lo que se requería demostrar para el examen) Para demostrar que $x$ sigue una distribución binomial, una forma (no es la única, otra la vimos en clase) es primero notar que el número de pasos hacia la izquierda o hacia la derecha, son una distribución binomial, pues se pueden tratar como tiros de Bernoulli. Si se dan $i$ pasos a la izquierda (con $i$ desde $0$ hasta $n$), se dará $n-i$ pasos a la derecha, por lo que en total avanzará $2i-n = x$ pasos. Por lo tanto, basta con que pensemos en la distribución binomial para tener $\\frac{x+n}{2}$ pasos (éxitos) hacia la izquirda para obtener la posición $x$, con $x$ desde $-n$ hasta $n$. Esta tendrá una distribución binomial (pues se trata simplemente de la distribución de los pasos hacia la izquierda) y nos dará la probabilidad de avanzar a la posición $x$. El valor esperado de esta distribución será 0, pues sería $\\langle x \\rangle = \\sum  (2i-n)p_X(i) = \\sum (2i-n)  \\binom {n}{i} p^{i} (1-p)^{n-i} =  \\sum i  \\binom {n}{i} p^{i} (1-p)^{n-i} -  \\sum (n-i)\\binom {n}{i} p^{i} (1-p)^{n-i} = \\langle x^+ \\rangle - \\langle x^- \\rangle = 0$. Por supuesto, no todas las $x$ son posibles, sino sólo donde $x+n$ es un número par, como era de esperarse. Es decir, se tiene que re-escalar el sistema a pasos de tamaño 2, pero donde la distribución está centrada en el origen, y no en $np$ (o sea, el valor esperado es 0 y no $np$). Puesto que se escala en 2 el tamaño de los pasos, la desviación estandar se escala también en 2 (se trata de una gaussiana del doble de ancho que la producida por la distribución binomial) y por lo tanto la varianza se escala por 4.  \n",
    "\n",
    "ii) Si agregamos la posibilidad de que el caminante se pueda detener, entonces simplemente reducimos la probabilidad $p$ de $\\frac{1}{2}$ a una cantidad menor; sin embargo, siguien siendo válidos todos los argumentos (siempre y cuando $p$ no sea demasiado cercano a 0). Por lo tanto el resultado se mantiene. \n",
    "\n",
    "iii) Para el caso $ND$, lo único que tenemos que notar es que $\\langle x^2 \\rangle = \\sum \\langle x_i^2 \\rangle$, donde $x_i$ son los desplazamientos sobre el eje $i$. Entonces, la probabilidad de que se mueva en cualquiera de los ejes en cada paso será $\\frac{1}{N}$, por lo que la probabilidad de que se mueva en una determinada dirección será $p = \\frac{1}{2N}$. Si $N$ no es demasiado grande, podemos ocupar lo del inciso anterior para decir que $\\langle x_i^2 \\rangle \\sim n$, lo que implica que la suma sobre todos los $i$ (como todos tienen igual probabilidad), sea $\\langle x^2 \\rangle = N \\langle x_i^2 \\rangle \\sim N n p\\sim n$ para una N constante. \n",
    "\n",
    "iv) La aproximación de la gaussiana es sólo válida si $n p^2 \\nsim 0$, o bien, si $n p^2 >> 0$, lo que implica que $\\frac{n}{4 N^2} >> 0 $. Si esto no pasara, entonces se tendría que usar la aproximación Poissoniana, en cuyo caso, la varianza va como $np$, en vez de $np^2$. Aún se mantiene el resultado de $\\langle x^2 \\rangle \\sim n$ para $N$ constante, sin embargo, si se varía $n$ y $N$ al mismo tiempo,  $\\langle x^2 \\rangle \\sim N n p^2 = \\frac{N n}{4 N^2} = \\frac{n}{4 N} $, es decir, el coeficiente de difusión va reduciéndose. \n",
    "\n",
    "III) Versión fácil: El sistema es simétrico, por lo que el valor esperado de la fuerza hacia la izquierda y hacia la derecha, deben ser iguales. Puesto que el valor esperado de la fuerza total es la la resta de la fuerza en cada una de las direcciones, el valor esperado total es 0. \n",
    "\n",
    "Versión larga: El valor esperado se define como $\\epsilon(x) = \\sum x P_X (x)$. Para cada dirección, la distribución, es una distribución binomial. Puesto que todas las probabilidades son iguales, $p=1/3$. Es decir, $P_X (x) = \\binom{n}{x} \\frac{1}{3^x} \\frac{2^{n-x}}{3^{n-x}} = \\binom{n}{x} \\frac{2^{n-x}}{3^n}$. Para cada valor de $x$ y $y$ de personas que se mueven a la izquierda y la derecha respectivamente, hay $n-x-y$ que se quedan en su mismo lugar. Así, para cada valor de $x$ y $y$, la fuerza total será: \n",
    "\n",
    "$F_t = 3^n (\\binom{n}{x} 2^{n-x} -\\binom{n}{y} 2^{n-y} + sig(\\binom{n}{x} 2^{n-x} -\\binom{n}{y} 2^{n-y} ) \\binom{n}{n-x-y} 2^{x+y})$.\n",
    "\n",
    "Si variamos $x$ de $0$ a $n$, $y$ sólo lo podemos variar de $0$ a $n-x$, debido a la constricción $x+y\\leq n$. \n",
    "\n",
    "Por lo tanto la fuerza total promedio será la suma de $F_t$ sobre $x$ y sobre $y$, es decir: \n",
    "\n",
    "$\\langle F \\rangle = 3^n \\sum_{x=0}^{n} \\sum_{y=0}^{n-x} (\\binom{n}{x} 2^{n-x} -\\binom{n}{y} 2^{n-y} + sig(\\binom{n}{x} 2^{n-x} -\\binom{n}{y} 2^{n-y} ) \\binom{n}{n-x-y} 2^{x+y})$\n",
    "\n",
    "(hasta aquí bastaba para el examen)\n",
    "\n",
    "Puesto que $x$ y $y$ van sobre exactamente los mismos valores, la primera parte de la suma es igual a 0. Es decir: \n",
    "\n",
    "$\\langle F \\rangle = 3^n \\sum_{x=0}^{n} \\sum_{y=0}^{n-x} sig(\\binom{n}{x} 2^{n-x} -\\binom{n}{y} 2^{n-y} ) \\binom{n}{n-x-y} 2^{x+y}$\n",
    "\n",
    "nuevamente, si el signo de $\\binom{n}{x} 2^{n-x} -\\binom{n}{y} 2^{n-y} $ es positivo para el par ordenado $(x_i,y_i)$, entonces, será negativo para el par $(y_i, x_i)$, pero $ \\binom{n}{n-x_i-y_i} 2^{x_i+y_i} =  \\binom{n}{n-y_i-x_i} 2^{y_i+x_i}$, por lo tanto, la última parte de la suma es también igual a 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.5",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
